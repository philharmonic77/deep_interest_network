{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2165db",
   "metadata": {},
   "source": [
    "**背景:**\n",
    "\n",
    "承接demo_v1，完善上一篇被简化的部分。逐步代码模块化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c15894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version of tensorflow: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"version of tensorflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb63e56",
   "metadata": {},
   "source": [
    "# 预备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4b249",
   "metadata": {},
   "source": [
    "## mask & padding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef38c8",
   "metadata": {},
   "source": [
    "对不等长的序列需要padding，一般是末尾用0补齐。  \n",
    "可以参考[官方guide](https://www.tensorflow.org/guide/keras/understanding_masking_and_padding#passing_mask_tensors_directly_to_layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7d0c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 711  632   71    0    0    0]\n",
      " [  73    8 3215   55  927    0]\n",
      " [  83   91    1  645 1253  927]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "]\n",
    "\n",
    "# tensorflow 2.12使用下面的代码\n",
    "# padded_inputs = tf.keras.utils.pad_sequences(\n",
    "#     raw_inputs, padding=\"post\"\n",
    "# )\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    raw_inputs, padding=\"post\"\n",
    ")\n",
    "print(padded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07251108",
   "metadata": {},
   "source": [
    "对序列做embedding，设置mask_zero=True，查看mask的效果。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be56469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 14:08:21.315221: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-05-29 14:08:21.316286: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "\n",
    "print(masked_output._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992d006",
   "metadata": {},
   "source": [
    "**备注：**  \n",
    "1. 在使用tf.keras的Sequence或者Functional API时，mask layer或者指明了mask的embedding layer的下游，只要支持mask都会自动使用这个信息。\n",
    "2. 可以认为embedding是mask的生产者，它实现了compute_mask方法供调用；rnn或者lstm等是mask的消费者，他们的__call__方法里支持mask参数，可以手动传进去。 \n",
    "3. 在自定义layer的时候，需要重新实现compute_mask方法，参考[官方文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#compute_mask)和 [源代码](https://github.com/keras-team/keras/blob/v2.12.0/keras/engine/base_layer.py#L976-L998).\n",
    "\n",
    "  \n",
    "**回到DIN的实现上，sequence特征涉及embedding之后的pooling操作，以及attention部分，都需要处理mask的问题。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb29d9f",
   "metadata": {},
   "source": [
    "## 规范feature column参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4e2bc",
   "metadata": {},
   "source": [
    "### dense feature \n",
    "这类特征可以直接作为模型的输入，或者先[分桶](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Discretization)然后再考虑是否embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "138f5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果不做处理\n",
    "age = {\n",
    "    \"name\": \"age\",\n",
    "    \"dtype\": \"float32\",\n",
    "    \"dim\": 1 # 1维\n",
    "}\n",
    "\n",
    "# 如果需要分桶再embedding\n",
    "age = {\n",
    "    \"name\": \"age\",\n",
    "    \"dtype\": \"float32\",\n",
    "    \"dim\": 1, # 1维\n",
    "    \n",
    "    \"use_bucket\": True,\n",
    "    \"bins\": [20, 30, 40, 50, 60], # 若use_bucket=False以下可以不填\n",
    "    \"emb_name\": \"age_emb\",\n",
    "    \"emb_dim\": 8\n",
    "}\n",
    "\n",
    "# 多维：view 各个cid的次数\n",
    "view_cid_cnt = {\n",
    "    \"name\": \"view_cid_cnt\",\n",
    "    \"dtype\": \"float32\",\n",
    "    \"dim\": 100 # 100维   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01a908",
   "metadata": {},
   "source": [
    "### sparse feature\n",
    "这类特征可以直接embedding，但是如果取值过多也可以先hash到有限个取值上。参考[这里](https://www.tensorflow.org/guide/keras/preprocessing_layers#applying_the_hashing_trick_to_an_integer_categorical_feature)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644bee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iid = {\n",
    "    \"name\": \"iid\",\n",
    "    \"dtype\": \"bytes\",\n",
    "    \n",
    "    \"vocab_size\": 100000,\n",
    "    \"use_hash\": True, # 若use_hash=False，hash_size可以不填\n",
    "    \"hash_size\": 10000,\n",
    "    \"emb_name\": \"iid_emb\",\n",
    "    \"emb_dim\": 1000    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0397c01",
   "metadata": {},
   "source": [
    "### seq/multi sparse feature\n",
    "这类特征dim>1，可能是sequence，也可以不是，eg：tag list。和sparse feature相比，需要多定义max_len, combiner(pooling参数)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156a30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_iid = {\n",
    "    \"name\": \"iid\",\n",
    "    \"dtype\": \"bytes\",\n",
    "    \"max_len\": 100,\n",
    "    \n",
    "    \"vocab_size\": 100000,\n",
    "    \"use_hash\": True,\n",
    "    \"hash_size\": 10000, # 若use_hash=False，hash_size可以不填\n",
    "    \"emb_name\": \"iid_emb\",\n",
    "    \"emb_dim\": 1000,\n",
    "    \n",
    "    \"need_padding\": True, # 如果输入不定长，则需要设置为True\n",
    "    \"combiner\": \"mean\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519bbc6",
   "metadata": {},
   "source": [
    "## 定义模型结构参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38781e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"user_feat\": [],\n",
    "    \"item_feat\": [],\n",
    "    \"mlp_hidden_size\": [128, 64, 64],\n",
    "    \"use_bn\": True,\n",
    "    \"din\": [\"view_iid|iid\", \"view_cid|cid\"], # 这里必须是seq特征｜item特征 \n",
    "    \"din_hidden_size\": [64, 32]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388e25e",
   "metadata": {},
   "source": [
    "# 代码模块化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a1448",
   "metadata": {},
   "source": [
    "## 定义feature column\n",
    "1. tensorflow 2.0以上，不再使用feature column，转而用tf.keras.layers。两者的对应关系可以查看[官方文档](https://www.tensorflow.org/guide/migrate/migrating_feature_columns#feature_column_equivalence_table). \n",
    "2. 三种特征类型这里使用namedtuple类来定义，可以参考namedtuple的[教程](https://realpython.com/python-namedtuple/#subclassing-namedtuple-classes). 这样做可以增加代码可读性，适用于定义属性多，方法少的类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d1c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cea24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dtype', 'dim', 'use_bucket', 'bins', 'emb_name', 'emb_dim'])):\n",
    "    \n",
    "    # use __slots__ = () to prevent the automatic creation of a per-instance __dict__\n",
    "    # keep memory efficient\n",
    "    __slots__ = () \n",
    "\n",
    "    # set default value of arguments\n",
    "    def __new__(cls, name, dtype=\"float32\", dim=1, use_bucket=False, bins=None, emb_name=None, emb_dim=None):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dtype, dim, use_bucket, bins, emb_name, emb_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef31069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseFeat(namedtuple('SparseFeat', ['name', 'vocab_size', 'emb_dim', 'dtype', 'use_hash', 'hash_size', 'emb_name'])):\n",
    "    \n",
    "    __slots__ = () \n",
    "    \n",
    "    def __new__(cls, name, vocab_size, emb_dim, dtype=\"int32\", use_hash=False, hash_size=None, emb_name=None):\n",
    "        \n",
    "        if emb_name is None:\n",
    "            emb_name = name\n",
    "\n",
    "        return super(SparseFeat, cls).__new__(cls, name, vocab_size, emb_dim, dtype, use_hash, hash_size, emb_name)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91950def",
   "metadata": {},
   "source": [
    "SeqSparseFeat是多个SparseFeat类采用嵌套的形式，需要解析出SparseFeat类的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0cfff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqSparseFeat(namedtuple('SeqSparseFeat', ['sparsefeat', 'max_len', 'need_padding', 'combiner'])):\n",
    "    \n",
    "    __slots__ = () \n",
    "    \n",
    "    def __new__ (cls, sparsefeat, max_len, need_padding=True, combiner='nean'):\n",
    "        return super(SeqSparseFeat, cls).__new__(cls, sparsefeat, maxlen, need_padding, combiner)\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.sparsefeat.vocab_size    \n",
    "    \n",
    "    @property\n",
    "    def emb_dim(self):\n",
    "        return self.sparsefeat.emb_dim  \n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype  \n",
    "    \n",
    "    @property\n",
    "    def use_hash(self):\n",
    "        return self.sparsefeat.use_hash  \n",
    "    \n",
    "    @property\n",
    "    def hash_size(self):\n",
    "        return self.sparsefeat.hash_size  \n",
    "    \n",
    "    @property\n",
    "    def emb_name(self):\n",
    "        return self.sparsefeat.emb_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25659c82",
   "metadata": {},
   "source": [
    "## 自定义mlp结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae8264",
   "metadata": {},
   "source": [
    "这部分是继承keras.layers，实现自定义的layer，可以参考[官方文档](https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing)，主要是希望代码更结构化更清晰。  \n",
    "自定义layer里关于get_config、from_config的说明可以查看[这里](https://www.tensorflow.org/guide/keras/serialization_and_saving#savedmodel_format)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d2f8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc225843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn  \n",
    "        self.seed = seed\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "                    \n",
    "        if l2_reg > 0:\n",
    "            self.mlp_layers = [layers.Dense(units=unit, activation=None, \n",
    "                                                kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)) for unit in self.hidden_units]\n",
    "        else:\n",
    "            self.mlp_layers = [layers.Dense(units=unit, activation=None) for unit in self.hidden_units]\n",
    "                \n",
    "        if self.use_bn:\n",
    "            self.bn_layers = [layers.BatchNormalization() for _ in range(len(self.hidden_units))]\n",
    "            \n",
    "        self.activation_layers = [layers.Activation(self.activation) for _ in range(len(self.hidden_units))]\n",
    "\n",
    "        self.dropout_layers = [layers.Dropout(self.dropout_rate, seed=self.seed + i) for i in\n",
    "                               range(len(self.hidden_units))]  \n",
    "        \n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            fc = self.mlp_layers[i]\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn_layers[i](fc, training=training)\n",
    "            \n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout_layers[i](fc, training=training)\n",
    "            \n",
    "            deep_input = fc\n",
    "            \n",
    "        return deep_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if len(self.hidden_units) > 0:\n",
    "            shape = input_shape[:-1] + (self.hidden_units[-1],)\n",
    "        else:\n",
    "            shape = input_shape\n",
    "\n",
    "        return tuple(shape)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
    "                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate, 'seed': self.seed}\n",
    "        base_config = super(MLP, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0ccb3",
   "metadata": {},
   "source": [
    "## 自定义din结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110abd1",
   "metadata": {},
   "source": [
    "最小激活单元，mlp结构，输出注意力得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c17b9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalActivationUnit(Layer):\n",
    "    \"\"\"\n",
    "      Input shape\n",
    "        - A list of two 3D tensor with shape:  ``(batch_size, 1, embedding_size)`` and ``(batch_size, T, embedding_size)``\n",
    "\n",
    "      Output shape : attention scores \n",
    "        - 3D tensor with shape: ``(batch_size, T, 1)``\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=(64, 32), activation='sigmoid', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024,\n",
    "                 **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.seed = seed\n",
    "        super(LocalActivationUnit, self).__init__(**kwargs)\n",
    "        self.supports_masking = True \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.mlp = MLP(self.hidden_units, self.activation, self.l2_reg, self.dropout_rate, self.use_bn, seed=self.seed)\n",
    "        self.out_layer = keras.Layer.Dense(units=1, activation='sigmoid') \n",
    "        \n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "\n",
    "        query, keys = inputs\n",
    "\n",
    "        keys_len = keys.get_shape()[1]\n",
    "        queries = K.repeat_elements(query, keys_len, 1)\n",
    "\n",
    "        att_input = tf.concat(\n",
    "            [queries, keys, queries - keys, queries * keys], axis=-1)\n",
    "\n",
    "        att_out = self.mlp(att_input, training=training)\n",
    "\n",
    "        attention_score = self.out_layer(att_out)\n",
    "\n",
    "        return attention_score   \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[1][:2] + (1,)\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return mask\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
    "                  'l2_reg': self.l2_reg, 'dropout_rate': self.dropout_rate, 'use_bn': self.use_bn, 'seed': self.seed}\n",
    "        base_config = super(LocalActivationUnit, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65cfa0",
   "metadata": {},
   "source": [
    "下面写完整的layer，这里supports_masking设置为True，注意input（这里input是embedding）一定要支持mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46eabe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSequencePoolingLayer(Layer):\n",
    "    \"\"\"The Attentional sequence pooling operation used in DIN.\n",
    "\n",
    "      Input shape\n",
    "        - A list of three tensor: [query,keys,keys_length]\n",
    "\n",
    "        - query is a 3D tensor with shape:  ``(batch_size, 1, embedding_size)``\n",
    "\n",
    "        - keys is a 3D tensor with shape:   ``(batch_size, T, embedding_size)``\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, att_hidden_units=(80, 40), att_activation='sigmoid', **kwargs):\n",
    "\n",
    "        self.att_hidden_units = att_hidden_units\n",
    "        self.att_activation = att_activation\n",
    "        super(AttentionSequencePoolingLayer, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.local_att = LocalActivationUnit(\n",
    "            self.att_hidden_units, self.att_activation, l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, )\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None, **kwargs):\n",
    "\n",
    "        queries, keys = inputs\n",
    "        key_masks = tf.expand_dims(mask[-1], axis=1)\n",
    "\n",
    "        attention_score = self.local_att([queries, keys], training=training)\n",
    "\n",
    "        outputs = tf.transpose(attention_score, (0, 2, 1))\n",
    "\n",
    "        outputs = tf.matmul(outputs, keys)\n",
    "\n",
    "        outputs._uses_learning_phase = training is not None\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1, input_shape[0][-1])\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None\n",
    "\n",
    "    def get_config(self, ):\n",
    "\n",
    "        config = {'att_hidden_units': self.att_hidden_units, 'att_activation': self.att_activation}\n",
    "        base_config = super(AttentionSequencePoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd1ae3",
   "metadata": {},
   "source": [
    "遗留flatten和pooling，之后再看是否需要自定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713cad5",
   "metadata": {},
   "source": [
    "# 模拟数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba450a",
   "metadata": {},
   "source": [
    "**和demo_v1相比，有以下变化：**\n",
    "1. 已经定义了feature column类，这里直接用这些类生成模拟数据。\n",
    "2. 保证每类特征至少有两个，以便更好的测试代码。\n",
    "3. 考虑序列特征的mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96805d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c27865",
   "metadata": {},
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "tf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
